# Applied Information Retrieval Assignments (Spring 2025)

This repository serves as a comprehensive showcase of coding assignments completed for the "Applied Information Retrieval" course (COMPSCI 546 - Spring 2025). It consolidates practical implementations and analytical solutions across a diverse range of topics, including information retrieval metrics, text processing, classic ranking models, relevance feedback, keyword extraction, clustering algorithms, learning-to-rank, and collaborative recommender systems.

## üöÄ Project Overview

The assignments primarily leverage **Python** for implementation, with development and execution typically performed within **Google Colab notebooks**. This project demonstrates proficiency in building, evaluating, and analyzing various information retrieval and recommendation systems. A foundational understanding of Python and relevant mathematical concepts in information retrieval is recommended to fully appreciate and extend these projects.

## üíª Repository Structure, Setup, and Execution

While the core code and execution environment for each assignment reside within its respective Google Colab notebook, this repository organizes links to these notebooks and provides high-level descriptions.

To interact with any assignment:

1.  **Access the Original Colab Link**: Navigate to the provided Google Colab link for the specific assignment.
2.  **Save to Google Drive**: It is crucial to save a copy of the notebook to your personal Google Drive. This can be done by clicking the "Copy to Drive" button or using "Share" and "Copy Link" from the Colab menu.
3.  **Download Input Files**: Most assignments include a dedicated cell within the Colab notebook to download necessary input data files (e.g., `.qrel` files, collection data) directly into the Colab runtime environment. This typically involves using `pydrive` to fetch a zipped archive (e.g., `HW01.zip`), extracting its contents into a working directory (e.g., `./HW01`), and then removing the zip file. For Assignment 8, the MovieLens 100K dataset is dynamically downloaded.
4.  **Execute Cells**: Run all cells in the notebook sequentially (`Runtime -> Run All`) to re-execute the code and generate outputs.
5.  **Code Implementation**: All functions are implemented from scratch without external libraries for core computations, unless explicitly stated (e.g., `pyltr` for Assignment 7, `TensorFlow` for Assignment 8).
   
## üìù Assignments

### Assignment 1: Information Retrieval Metrics

*   **What it Solves**: This assignment focuses on implementing and executing Python code to **evaluate the ranked outputs generated by a retrieval model or a recommender system**. It provides a fundamental toolkit for quantifying the effectiveness of such systems.
*   **Key Concepts Implemented**:
    *   **Precision@k (P@k)**: Measures the percentage of relevant documents within the top `k` results, averaged across `n` queries.
    *   **Recall@k (R@k)**: Calculates the ratio of relevant documents found among the top `k` results compared to the total number of relevant documents, averaged across `n` queries.
    *   **F1 Measure@k (F1@k)**: Combines precision and recall into a single score, considering only the top `k` results for each query, averaged across `n` queries.
    *   **Mean Reciprocal Rank (MRR@k)**: Evaluates the rank of the first relevant result within the top `k` retrieved responses across various queries. The Reciprocal Rank for a single query is defined as `1/r`, where `r` is the rank of the first relevant result, or `0` if no relevant results are found.
    *   **Mean Average Precision (MAP@k)**: Takes into account precision at various ranks and the availability of relevant documents among the top `k` retrieved results for multiple queries. For a single query, Average Precision (AP@k) is computed as `sum(P@j ‚ãÖ rel(j)) / R`, where `P@j` is precision at rank `j`, `rel(j)` is an indicator function (1 if relevant, 0 otherwise), and `R` is the total number of relevant documents for the query.
    *   **Normalized Discounted Cumulative Gain (NDCG@k)**: Considers graded relevance judgments, rewarding highly relevant documents appearing earlier in the top `k` ranks. Relevance labels (1-4) are mapped to gains (0-3) for this calculation. It involves computing Discounted Cumulative Gain (DCG@k), Ideal DCG (IDCG@k), and then normalizing `NDCG@k = DCG@k / IDCG@k`.
*   **Input Files**:
    *   `antique-train-final.qrel`: Contains relevance judgments (ground truth) where `[queryid] [topicid] [passageid] [relevancejudgment]`. Relevance labels range from 1 (Non-Relevant) to 4 (Highly Relevant), with labels 3 and 4 considered relevant for binary metrics. If a query-document pair is not listed, it's assumed non-relevant.
    *   `ranking_file`: Contains ranked outputs (top 100 passages) from a standard search engine for train queries, including `[queryid] [topicid] [passageid] [rank] [relevance_score] [indri]`. For this assignment, only `queryid` and `passageid` are primarily needed, and passages are already ranked by relevance score.
*   **Code Reference**: Functions such as `loadQrels`, `loadRankfile`, `calcPrecision`, `calcRecall`, `calcFScore`, `calcMRR`, `calcMAP`, and `calcNDCG`.
*   **Sample Outputs**:
    *   Total Num of queries in the qrel file: `2426`
    *   Total Num of queries in the rank file: `2426`
    *   Total number of relevant passages: `19813`
    *   Precision at top 10: `0.10729596042868805`
    *   Recall at top 5: `0.12934416493801082`
    *   F1 score at top 5: `0.12587171180786325`
    *   MRR at top 5: `0.3375996152789238`
    *   MAP at top 50: `0.12134118627027757`
    *   NDCG at top 5: `0.19759308493592626`
*   **Original Colab Link**: https://colab.research.google.com/drive/1K4MFsgxZeMwYdNIxkfEFrTCDIle19PAT?usp=sharing

### Assignment 2: Text Processing and Indexing

*   **What it Solves**: This assignment implements an **inverted index with count information**, a core component of most information retrieval systems. It also demonstrates data compression via delta encoding and verifies Zipf's Law in a real-world corpus.
*   **Key Concepts Implemented**:
    *   **Inverted Index Construction**: Creation of an inverted index that stores term frequencies within passages. This involves mapping original passage IDs to new integer IDs (1 to N), and storing statistics like total tokens, unique terms, and passages. Passage IDs in postings lists are sorted in ascending order.
    *   **Delta Encoding**: Implementation of a compression method for passage IDs in the inverted index, storing differences ("deltas") between consecutive IDs instead of the actual values to reduce storage space. This is applied only to passage IDs, not term count information.
    *   **Zipf's Law**: Verification and plotting of the empirical rule that describes the inverse relationship between term frequency and its rank in a corpus. It demonstrates that the product of a term's frequency and its rank remains approximately constant, though common and rare terms might deviate.
*   **Input Files**:
    *   `antique-collection.tok.clean_kstem`: A collection of passages from the ANTIQUE dataset, with each row containing `passageId` and `passageText` tab-separated.
*   **Code Reference**: Functions `createIndexCount`, `createIndexDeltaCount`, `accessTermInfoIndex`, `accessTermInfoIndexDelta`, `displayIndexStats`, `displayIndexDeltaStats`, `zipf`, and `plotZipf`.
*   **Sample Outputs**:
    *   Total number of passages in the collection: `403492`
    *   First 5 passages with the term "computer": `['629558_0', '1528314_0', '3384343_0', '1873488_1', '2084699_0']`
    *   Total number of Unique Terms: `149467`
    *   Total number of tokens: `16590057`
    *   A Zipf's Law plot displaying log(probability) vs. log(rank) for all terms.
*   **Original Colab Link**: https://colab.research.google.com/drive/1QiKELyNXXgilKmEhBhAw0LvRf13gcFqf?usp=sharing

### Assignment 3: Retrieval Models

*   **What it Solves**: This assignment implements **three fundamental retrieval models** (Vector Space Model, BM25, and Query Likelihood with Jelinek-Mercer smoothing) and evaluates their performance in ranking passages for given queries.
*   **Key Concepts Implemented**:
    *   **Vector Space Model (VSM)**: Scores passages based on term frequency, inverse document frequency (IDF), and length normalization. It returns the top `k` results using a "term at a time" scoring method. The score function is `sum(count(w,q) * ln(1 + ln(1 + count(w,p))) * IDF / (1 - b + b * (|p|/avgdl)))` for terms `w` in `q` and `p`.
    *   **BM25**: A widely used ranking function that incorporates term frequency saturation, document length normalization, and inverse document frequency to score passages, returning the top `k` results. It iterates over all query tokens, including repetitions.
    *   **Query Likelihood Model with Jelinek-Mercer (JM) Smoothing**: Calculates passage scores based on the probability of query terms given the passage and the collection, smoothed using the Jelinek-Mercer method. The score is computed as `sum(ln((1 ‚àí Œª) * PMLE(w|p) + Œª * PMLE(w|C)))` for terms `w` in `q`, where `Œª` is set to 0.2. It iterates over all query tokens, including repetitions.
    *   **Evaluation**: Reuses and modifies Precision@k and Recall@k metrics (from Assignment 1) to evaluate the performance of the implemented models for the top 5 retrieved passages.
*   **Input Files**:
    *   `queries_tok_clean_kstem`: Sampled queries from the ANTIQUE dataset (`queryid` `query_text` tab-separated).
    *   `antique-collection.tok.clean_kstem`: Passage collection (`passage_id` `passage_text` tab-separated).
    *   `test.qrel`: Query relevance judgments, adhering to the format of Assignment 1 (`queryid topicid passageid relevance_judgement`).
*   **Code Reference**: Functions `loadQueries`, `loadQrels`, an `indexCount` class, `vsm`, `bm25`, `ql_jm` (with `convert_index` helper), and evaluation functions `calcPrecision` and `calcRecall`.
*   **Sample Outputs**:
    *   Total Num of queries in the query file: `15`
    *   Total Num of queries in the qrel file: `15`
    *   Total number of passages in the collection: `403492`
    *   The top retrieved passage and score for query id "3698636" using VSM: `[('754739_3', 13.04698249124851)]`
    *   The top retrieved passage and score for query id "3698636" using BM25: `[('3698636_9', 18.355784008150206)]`
    *   Precision at top 5 (BM25): `0.36000000000000004`
    *   Recall at top 5 (BM25): `0.12100747347124159`
*   **Original Colab Link**: https://colab.research.google.com/drive/1LMmSPiTKVgc3A31iObOVZ1UYBQJdOfDo?usp=sharing

### Assignment 4: Relevance Models

*   **What it Solves**: This assignment implements the **RM3 model for Query Expansion**, a technique used to refine initial queries by incorporating terms from highly-ranked feedback passages, thereby improving retrieval effectiveness.
*   **Key Concepts Implemented**:
    *   **Maximum Likelihood Estimates (MLE) for Query Language Model**: `PMLE(t|q) = count(t,q) / |q|`, where `count(t,q)` is the number of times term `t` appears in query `q`, and `|q|` is the number of tokens in query `q`. These estimates are calculated for all queries.
    *   **RM1 Feedback Model**: Implementation of the RM1 model to compute term probabilities from a set of feedback passages (top 10 passages). The formula is `PRM1(t|Œ∏F) ‚àù sum(p(t|Œ∏p) * product(p(w|Œ∏p) for w in q))`. `p(t|Œ∏p)` is smoothed using Dirichlet smoothing: `(count(t,p) + Œ¥) / (|p| + Œ¥|V|)`, where `Œ¥ = 0.1` and `|V|` is the vocabulary size. The computed weights for each term are normalized to sum to 1.
    *   **RM3 Feedback Model**: Implementation of RM3, a hybrid model that combines the probabilities derived from the RM1 model with the initial query MLE probabilities using a mixing parameter `Œ± = 0.5`. The formula is `PRM3(t|Œ∏F) = Œ± * PRM1(t|Œ∏F) + (1 ‚àí Œ±) * PMLE(t|q)`. This is computed for all unique terms present in both feedback passages and the query text.
*   **Input Files**:
    *   `queries_tok_clean_kstem`: Sampled queries from the ANTIQUE dataset (`query_id` `query_text` tab-separated).
    *   `antique-collection.tok.clean_kstem`: Passage collection (`passage_id` `passage_text` tab-separated).
    *   `query_pass`: A file containing queries and 10 feedback passages corresponding to each query (`query_id passage_id1 ... passage_id10` space-separated).
*   **Code Reference**: Functions `loadQueries`, `loadFeedbackPass`, `storeFeedbackPass`, an `indexCount` class (modified to include `feedback_pass_contents`), `calcMleQueries`, `calcRM1`, and `calcRM3`.
*   **Sample Outputs**:
    *   Total Num of queries in the query file: `5`
    *   Total Num of queries in the feedback file: `5`
    *   Total number of passages in the collection: `403492`
    *   MLE estimates for qid 3396066: `{'why': 0.16666..., 'do': 0.16666..., ...}`
    *   Top 20 Feedback terms and their RM1 probabilities for qid 3396066: `['the:0.0820...', 'to:0.0388...', ...]`
    *   Top 20 Feedback terms and their RM3 probabilities for qid 3396066: `['fly:0.0873...', 'so:0.0862...', ...]`
*   **Original Colab Link**: https://colab.research.google.com/drive/19yjJRZZW69qw_JxaELACEAU0BhUzJWiz?usp=sharing

### Assignment 5: TextRank

*   **What it Solves**: This assignment implements the **TextRank Algorithm for keyword extraction** from a document. It effectively identifies the most important terms within a given text by modeling word co-occurrence as a graph and applying a graph-based ranking algorithm.
*   **Key Concepts Implemented**:
    *   **Graph Construction**: Creation of an unweighted and undirected graph where nodes represent terms, and an edge between terms implies they co-occur within a defined window size (`w=2`). This involves generating a vocabulary (mapping terms to unique integer IDs) and creating a list of term-pairs.
    *   **Transition Matrix**: Generation and normalization of a square matrix `M` (where `n` is `vocab_size`). Each cell `M_i,j` is 1 if the `i`-th and `j`-th words co-occur within the window, and 0 otherwise. Since the graph is undirected, `M_i,j = M_j,i`. The matrix is then normalized such that the sum of elements in each row is 1 (`sum(M_i,j for j) = 1`).
    *   **PageRank Algorithm**: Execution of the iterative PageRank algorithm on the constructed term graph for 50 iterations. The initial PageRank vector `p0` is a randomly initialized `n x 1` matrix whose elements sum to 1. The random jump probability `Œ±` is set to `0.15`. The PageRank vector is updated iteratively using the equation: `p(t+1) = (Œ±/n) + ((1 ‚àí Œ±) * M^T * p(t))`. After convergence, the top 10 terms with the highest PageRank scores are displayed as keywords.
    *   **Explanation of PageRank**: Discussion on why PageRank considers indirect links to assess importance and why terms with high PageRank scores in a co-occurrence graph are effective keywords. High PageRank indicates central terms frequently co-occurring with other important terms, capturing main ideas.
*   **Input Files**:
    *   `covid_nyt.tok.clean_nostop`: A single pre-processed news article document about Covid (punctuation and non-alphanumeric characters removed, tokenized, stemmed, stopwords removed), with terms space-separated.
*   **Code Reference**: Functions `genInit`, `createMatrix`, `normalizeMatric`, and `pageRank`.
*   **Sample Outputs**:
    *   Total number of unique terms in the collection: `1640`
    *   Total number of term pairs: `3464`
    *   Shape of the transition matrix: `(1640, 1640)`
    *   Top keyword: `said`, Weight: `0.009333873477753583`
    *   10th top keyword: `also`, Weight: `0.004257943677302679`
*   **Original Colab Link**: https://colab.research.google.com/drive/14u41wuU9Z2-E28VY2I1TGH2IT_AFoYHx?usp=sharing

### Assignment 6: Clustering

*   **What it Solves**: This assignment implements the **k-means clustering algorithm to group passages**, demonstrating unsupervised learning for document organization and analysis. It also includes methods for evaluating clustering quality and selecting an optimal number of clusters.
*   **Key Concepts Implemented**:
    *   **Passage Representation (TF-IDF)**: Each passage is converted into a vector representation. Every passage is represented by a vector with dimensionality equal to `vocab_size`. Each dimension corresponds to a word, and its value is the TF-IDF score of that word in the passage. The TF-IDF is calculated as `ln(1 + count(i,j)) * ln(1 + num_passages / df(j))`.
    *   **K-Means Algorithm**: Implementation for `k=3` clusters, where initial centroids are provided. The distance metric used is the squared Euclidean distance. The algorithm is executed for 30 iterations, involving assigning each passage to its nearest cluster and then recomputing cluster centroids.
    *   **IntraCluster Similarity (Average Diameter Distance)**: An evaluation metric for clustering compactness. It is calculated for each cluster as `sum(dist(x,y) for x,y in S, x!=y) / (|S|*(|S|-1))`, where `S` is the set of passages in a cluster and `dist(x,y)` is the squared Euclidean distance between passages `x` and `y`.
    *   **Intercluster Similarity (Average Linkage Distance)**: An evaluation metric for cluster separation. It is calculated for every pair of clusters `S` and `T` as `sum(dist(x,y) for x in S, y in T) / (|S|*|T|)`.
    *   **Approximating `k`**: Description of the "Elbow Method" using Residual Sum of Squares (RSS) to find a good approximation for the number of clusters in a dataset. This involves plotting RSS against various values of `k` and identifying where the curve flattens.
    *   **Improved Seed Selection**: Discussion of strategies to mitigate the dependency on initial random centroids for k-means, such as excluding outliers, trying multiple starting points, and initializing with other clustering methods.
*   **Input Files**:
    *   `passages.tok.clean_kstem`: A collection of passages, with `passage_id` and `passage_text` tab-separated.
    *   `centroid.txt`: Initial centroid vector values for the k-means algorithm, with one cluster vector per row.
*   **Code Reference**: Functions `load_centroid_init`, `load_vocab`, `create_input_matrix`, `init_centers`, `kmeans`, `average_diameter_dist`, and `average_linkage_dist`.
*   **Sample Outputs**:
    *   Vocabulary Size: `3630`
    *   Total Number of Passages: `500`
    *   Shape of the input matrix: `(500, 3630)`
    *   Number of initial centroids: `3`
    *   Number of elements in each cluster after final iteration: `0 363`, `1 128`, `2 9`
    *   Intracluster Similarity (Avg Diameter Dist) for cluster 0: `310.2319...`
    *   Intercluster Similarity (Avg Linkage Dist) for (0, 1): `746.0308...`
*   **Original Colab Link**: https://colab.research.google.com/drive/1o7MR1h5BHcnB0cUdopBdLMtDatLhHK43?usp=sharing

### Assignment 7: Learning to Rank & Neural Ranking Model

*   **What it Solves**: This assignment combines practical **Learning to Rank (LTR)** model training with conceptual understanding of **neural ranking models**. It involves feature engineering, training a LambdaMART model, and analyzing the theoretical underpinnings and architectures of BERT-based re-ranking/retrieval systems.
*   **Key Concepts Implemented**:
    *   **Feature Engineering**: Creation of custom features for query-passage pairs, including: the number of unique term overlaps (excluding stopwords and single-character words), the number of terms in the query, and the number of terms in the passage. These features are generated in a format compatible with the `pyltr` framework.
    *   **LambdaMART**: Utilizes the `pyltr` framework to train a LambdaMART model, a pairwise LTR algorithm that optimizes ranking metrics like NDCG using "lambdas." LambdaMART is described as a Multiple Additive Regression Trees (MART) model that uses an ensemble of decision trees and adjusts splits based on the gradient of ranking metrics.
    *   **Direct Optimization of Ranking Metrics**: Explanation of why typical ranking metrics (NDCG, MAP) cannot be directly optimized. This is because these metrics are dependent on the order of ranked items, making them discontinuous and non-differentiable with respect to ranking function parameters, necessitating surrogate loss functions.
    *   **BERT Re-Ranking Model Architecture**: Detailed description of a BERT-based re-ranking model, covering:
        *   **Input Construction**: Queries and passages are concatenated into a single sequence: `[CLS] query tokens [SEP] passage tokens [SEP]`. This is tokenized into input IDs, attention masks, and token type IDs.
        *   **BERT Encoder**: The tokenized sequence is passed through multiple layers of Transformer encoders, generating contextualized embeddings.
        *   **Pooling and Prediction**: The embedding corresponding to the `[CLS]` token is extracted, representing the combined meaning, and fed into a fully connected dense layer to produce a single scalar relevance score.
    *   **Pointwise vs. Pairwise Loss Functions**:
        *   **Pointwise Loss**: Treats relevance prediction as a regression or classification problem for individual query-passage pairs (e.g., Mean Squared Error).
        *   **Pairwise Loss**: Focuses on the relative order between two passages for the same query (e.g., Hinge Loss).
        *   **Expected Performance**: Pairwise loss functions are expected to perform better because they directly model the ranking objective by optimizing for correct ordering, aligning more closely with evaluation metrics like NDCG and MAP.
    *   **BERT for Retrieval (High-level design)**: For retrieval in large document collections (instead of re-ranking), the model can be designed to encode queries and documents independently into dense vector representations. Document embeddings can be pre-computed and stored, allowing retrieval via nearest neighbor search (e.g., using dot product or cosine similarity) between the query embedding and stored document embeddings.
*   **Input Files**:
    *   `antique-collection.tok.clean_kstem`: Passage collection.
    *   `antique-test-queries.tok.clean_kstem`, `antique-train-queries.tok.clean_kstem`, `antique-val-queries.tok.clean_kstem`: Query files for different data splits.
    *   `stopword_INQUERY`: List of stopwords, with one stopword per line.
    *   `val_baseline_features_top10`, `test_baseline_features_top10`, `train_baseline_features_top10`: Baseline feature files for respective splits (`query_id passage_id relevance_score vsm_score bm25_score`).
    *   `sample.txt`: Reference for the required output feature file format (`relevance_score qid:query_id 1:feature1 2:feature2 #docid = passage_id`).
*   **Code Reference**: Functions `loadQueryFile`, `loadCollection`, `loadStopWords`, and `featureCreation`. The `pyltr` library is used for LambdaMART model training.
*   **Sample Outputs**:
    *   Total Number of train queries: `2226`
    *   Total Number of passages in the collection: `403492`
    *   LambdaMART model test score: `0.8012772076456909`
*   **Original Colab Link**: https://colab.research.google.com/drive/1IPK8U7h-g3a88gWyTreODOrYRylIc4QJ?usp=sharing

### Assignment 8: Collaborative Recommender Systems

*   **What it Solves**: This assignment explores **Collaborative Filtering (CF) recommender systems** using the MovieLens 100K dataset. It implements a matrix factorization model using TensorFlow, enables personalized recommendations based on custom user inputs, and delves into the analytical aspects of embeddings and domain-specific considerations for recommenders. This assignment is a modified version of a Colab notebook originally created and distributed by Google as part of "Recommender Systems with TensorFlow".
*   **Key Concepts Implemented**:
    *   **Matrix Factorization**: Factorizing the sparse user-item ratings matrix `A` into lower-dimensional user embedding (`U`) and movie embedding (`V`) matrices, such that `A ‚âà UV^‚ä§`. Each user `i` is represented by a `d`-dimensional vector `U_i`, and each movie `j` by `V_j`, with the predicted rating being their dot product `‚ü®U_i, V_j‚ü©`.
    *   **Sparse Representation**: Efficiently representing the large, sparse ratings matrix using `tf.SparseTensor`, defined by its `indices`, `values`, and `dense_shape`.
    *   **Mean Squared Error (MSE)**: Calculation of MSE for *observed entries only* to measure the approximation error between true and predicted ratings in the sparse matrix.
    *   **Custom Ratings**: Functionality to allow users to add their own movie ratings (values between 1 and 5 inclusive) to the dataset to receive personalized recommendations.
    *   **`CFModel` Class**: A helper class that encapsulates the collaborative filtering model, managing embedding variables, defining the loss function, and tracking metrics during training. It provides access to trained embeddings via `model.embeddings`.
    *   **Embedding Inspection - Similarity Measures**:
        *   **Dot Product**: Computes the score of item `j` as `‚ü®u,Vj‚ü©`.
        *   **Cosine Similarity**: Computes the score of item `j` as `‚ü®u,Vj‚ü© / (‚à•u‚à•‚à•Vj‚à•)`.
        *   **Difference in Recommendations**: Dot product often leads to recommendations of popular movies, as the norm of embeddings in matrix factorization models can correlate with popularity.
    *   **Cold Start Problem**: Explanation of the challenge of making recommendations for new users or items due to a lack of historical interaction data.
        *   **Solutions**: Content-based filtering (using item features/user attributes), hybrid approaches, onboarding strategies (asking new users for initial ratings), and demographic or popularity-based defaults.
    *   **Music Domain Modifications**: Discussion on how to adapt CF models for music recommendations, considering the common pattern of frequent re-listening. Proposed modifications include: using implicit feedback (play counts, skips) instead of explicit ratings, implementing weighted matrix factorization (giving higher weight to frequently listened songs), and utilizing sequence models (like RNNs) to capture evolving user taste and real-time content.
*   **Input Files**:
    *   MovieLens 100K dataset: Downloaded dynamically within the notebook, consisting of 100,000 ratings from 943 users for 1682 movies.
*   **Code Reference**: Functions `build_rating_sparse_tensor`, `sparse_mean_square_error`, `CFModel` class, `build_model`, `compute_scores`, `user_recommendations`, and `movie_neighbors`. TensorFlow functions are utilized throughout.
*   **Sample Outputs**:
    *   Dataset contains: `b'943 users\n1682 items\n100000 ratings\n'`
    *   Added your `529` ratings
    *   `iteration 1000: train_error=0.378332, test_error=1.352503`
    *   User recommendations (Dot product, k=5): `Dream With the Fishes (1997)` (score: `8.351`)
    *   User recommendations (Cosine, k=5): `Time Tracers (1995)` (score: `0.599`)
    *   Nearest neighbors of `Braveheart (1995)` (Dot product, k=5): `Shawshank Redemption, The (1994)` (score: `6.209`)
*   **Original Colab Link**: https://colab.research.google.com/drive/1n5_w-WYii1T9IZoDIaIkXGCgF-PNiyiC?usp=sharing

---

## üìö Academic Honesty

These assignments were completed while adhering to specific academic honesty guidelines. This repository serves as a public demonstration of learned concepts and implemented solutions. Users are expected to follow their own institution's policies if utilizing this material for educational purposes. For the course, collaboration guidelines were to be strictly observed.

## üôè Acknowledgments

*   **Google Colab**: The primary development environment used for all assignments.
*   **MovieLens 100K dataset**: Utilized in Assignment 8.
*   **ANTIQUE dataset**: A passage retrieval dataset used across multiple information retrieval assignments (Assignments 1-4, 7).
*   **pyltr framework**: Employed in Assignment 7 for Learning to Rank.
*   **Recommender Systems with TensorFlow**: Assignment 8 is a modified version of a Colab notebook distributed by Google as part of this resource.

---
